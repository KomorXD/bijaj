# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H0_nDoW1xuwkNn5WoZaxgjuvzz68-I8_
"""
from PIL import ImageOps, Image

"""
!unzip dataset.zip

!pip install -U segmentation-models-pytorch albumentations --user 
#!pip uninstall -y segmentation-models-pytorch
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import numpy as np
import cv2
import matplotlib.pyplot as plt

TRAIN_IMG_DIR = "./train"
TRAIN_MASKS_DIR = "./train_masks"

VALID_IMG_DIR = "./valid"
VALID_MASKS_DIR = "./valid_masks"

TEST_IMG_DIR = "./test"
TEST_MASKS_DIR = "./test_masks"


def save_results(test_visualize, test_dataset, model, count=10):
    for i in range(count):
        n = np.random.choice(len(test_dataset))

        image_vis = test_visualize[n][0].astype('uint8')
        image, gt_mask = test_dataset[n]

        x_tensor = torch.from_numpy(image).to("cuda").unsqueeze(0)
        pr_mask = model.predict(x_tensor)
        pr_mask = (pr_mask.squeeze().cpu().numpy().round())

        # Convert images and masks to PTL images
        normal_image = Image.fromarray(np.uint8(image_vis)).resize((320, 320))

        gt_animals = Image.fromarray(np.uint8(gt_mask[0, ...].squeeze() * 255)).convert('RGB')
        gt_masking_background = Image.fromarray(np.uint8(gt_mask[1, ...].squeeze() * 255)).convert('RGB')
        gt_nonmasking_background = Image.fromarray(np.uint8(gt_mask[2, ...].squeeze() * 255)).convert('RGB')
        gt_foreground_attention = Image.fromarray(np.uint8(gt_mask[3, ...].squeeze() * 255)).convert('RGB')
        pr_animals = Image.fromarray(np.uint8(pr_mask[0, ...].squeeze() * 255)).convert('RGB')
        pr_masking_background = Image.fromarray(np.uint8(pr_mask[1, ...].squeeze() * 255)).convert('RGB')
        pr_nonmasking_background = Image.fromarray(np.uint8(pr_mask[2, ...].squeeze() * 255)).convert('RGB')
        pr_foreground_attention = Image.fromarray(np.uint8(pr_mask[3, ...].squeeze() * 255)).convert('RGB')

        # Create canvas and resize default image
        # normal_image = normal_image.resize((normal_image.width * 2, normal_image.height * 2))
        normal_image = normal_image.resize((normal_image.width * 2, normal_image.height * 2))
        new_image = Image.new('RGB', (normal_image.width * 3, normal_image.height))

        block_width = int(normal_image.width / 2)
        block_height = int(normal_image.height / 2)
        new_image.paste(normal_image, (0, 0))

        # Add red border to each image
        gt_animals = ImageOps.expand(gt_animals, border=1, fill='red')
        gt_masking_background = ImageOps.expand(gt_masking_background, border=1, fill='red')
        gt_nonmasking_background = ImageOps.expand(gt_nonmasking_background, border=1, fill='red')
        gt_foreground_attention = ImageOps.expand(gt_foreground_attention, border=1, fill='red')
        pr_animals = ImageOps.expand(pr_animals, border=1, fill='red')
        pr_masking_background = ImageOps.expand(pr_masking_background, border=1, fill='red')
        pr_nonmasking_background = ImageOps.expand(pr_nonmasking_background, border=1, fill='red')
        pr_foreground_attention = ImageOps.expand(pr_foreground_attention, border=1, fill='red')
        # Paste images on canvas to be merged
        new_image.paste(gt_animals, (block_width * 2, 0))
        new_image.paste(gt_masking_background, (block_width * 3, 0))
        new_image.paste(gt_nonmasking_background, (block_width * 4, 0))
        new_image.paste(gt_foreground_attention, (block_width * 5, 0))

        new_image.paste(pr_animals, (block_width * 2, block_height))
        new_image.paste(pr_masking_background, (block_width * 3, block_height))
        new_image.paste(pr_nonmasking_background, (block_width * 4, block_height))
        new_image.paste(pr_foreground_attention, (block_width * 5, block_height))

        # Save file
        new_image.save(f".\\test_results\\xd-{i}.png")


def visualize(**images):
    """PLot images in one row."""
    n = len(images)
    plt.figure(figsize=(16, 5))
    for i, (name, image) in enumerate(images.items()):
        plt.subplot(1, n, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.title(name)
        plt.imshow(image)
    plt.show()


from torch.utils.data import DataLoader
from torch.utils.data import Dataset as BaseDataset
from segmentation_models_pytorch import utils


class Dataset(BaseDataset):
    """CamVid Dataset. Read images, apply augmentation and preprocessing transformations.

    Args:
        images_dir (str): path to images folder
        masks_dir (str): path to segmentation masks folder
        class_values (list): values of classes to extract from segmentation mask
        augmentation (albumentations.Compose): data transfromation pipeline
            (e.g. flip, scale, etc.)
        preprocessing (albumentations.Compose): data preprocessing
            (e.g. noralization, shape manipulation, etc.)

    """

    CLASSES = [
        "Animal",
        "MaskingBackground",
        "NonMaskingBackground",
        "NonMaskingForegroundAttention",
        "None",
    ]

    COLORS = {
        0: (0, 0, 255),
        1: (0, 255, 0),
        2: (255, 0, 0),
        3: (255, 255, 255),
        4: (0, 0, 0),
    }

    def __init__(
            self,
            images_dir,
            masks_dir,
            classes=None,
            augmentation=None,
            preprocessing=None,
    ):
        self.ImagesIds = sorted(os.listdir(images_dir))
        self.MasksIds = sorted(os.listdir(masks_dir))
        self.ids = self.MasksIds
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ImagesIds]
        self.masks_fps = [os.path.join(masks_dir, mask_id) for mask_id in self.MasksIds]

        # convert str names to class values on masks
        self.class_values = [self.CLASSES.index(cls) for cls in classes]

        self.augmentation = augmentation
        self.preprocessing = preprocessing

    def __getitem__(self, i):
        # read data
        image = cv2.imread(self.images_fps[i])
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask = cv2.imread(self.masks_fps[i])
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)

        # Get the width and height of the image
        iheight, iwidth, _ = image.shape
        mheight, mwidth, _ = mask.shape

        # Calculate the new width and height that are divisible by 32
        inew_width = (iwidth // 32) * 32
        inew_height = (iheight // 32) * 32

        mnew_width = (mwidth // 32) * 32
        mnew_height = (mheight // 32) * 32

        image = cv2.resize(image, (inew_width, inew_height), interpolation=cv2.INTER_AREA)
        mask = cv2.resize(mask, (mnew_width, mnew_height), interpolation=cv2.INTER_AREA)

        # Crop the image to the new width and height
        cropped_image = image[0:inew_height, 0:inew_width]

        # extract certain classes from mask (e.g. cars)
        masks = []
        for cls_value in self.class_values:
            color = self.COLORS[cls_value]
            newMask = cv2.inRange(mask, color, color)
            newMask = np.float32(newMask)
            newMask = cv2.normalize(newMask, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)
            masks.append(newMask)
        mask = np.stack(masks, axis=-1).astype('float32')
        
        # apply augmentations
        if self.augmentation:
            sample = self.augmentation(image=image, mask=mask)
            image, mask = sample['image'], sample['mask']
        
        # apply preprocessing
        if self.preprocessing:
            sample = self.preprocessing(image=image, mask=mask)
            image, mask = sample['image'], sample['mask']
            
        return image, mask
        
    def __len__(self):
        return len(self.ids)


import albumentations as albu


def get_training_augmentation():
    train_transform = [

        albu.HorizontalFlip(p=0.5),

        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.8, border_mode=0),

        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),
        albu.RandomCrop(height=320, width=320, always_apply=True),

        albu.GaussNoise(p=0.2),

        albu.OneOf(
            [
                albu.CLAHE(p=0.33),
                albu.RandomBrightness(p=0.33),
                albu.RandomGamma(p=0.33),
            ],
            p=0.9,
        ),

        albu.OneOf(
            [
                albu.Sharpen(p=0.5),
                albu.Blur(blur_limit=(3, 5), p=1),
                albu.MotionBlur(blur_limit=(3, 5), p=0.5),
            ],
            p=0.9,
        ),

        albu.OneOf(
            [
                albu.RandomContrast(p=0.5),
                albu.HueSaturationValue(p=0.5),
            ],
            p=0.9,
        ),
    ]
    return albu.Compose(train_transform)


def get_validation_augmentation():
    """Add paddings to make image shape divisible by 32"""
    test_transform = [
        albu.PadIfNeeded(320, 320),
        albu.Resize(320, 320)
    ]
    return albu.Compose(test_transform)


def to_tensor(x, **kwargs):
    return x.transpose(2, 0, 1).astype('float32')


def get_preprocessing(preprocessing_fn):
    """Construct preprocessing transform

    Args:
        preprocessing_fn (callbale): data normalization function
            (can be specific for each pretrained neural network)
    Return:
        transform: albumentations.Compose

    """

    _transform = [
        albu.Lambda(image=preprocessing_fn),
        albu.Lambda(image=to_tensor, mask=to_tensor),
    ]
    return albu.Compose(_transform)


import torch
import numpy as np
import segmentation_models_pytorch as smp


def main():

    import os, ssl
    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and
    getattr(ssl, '_create_unverified_context', None)):
      ssl._create_default_https_context = ssl._create_unverified_context

    ENCODER = 'se_resnext50_32x4d'
    ENCODER_WEIGHTS = 'imagenet'
    CLASSES = ['Animal', 'NonMaskingBackground', "MaskingBackground", "NonMaskingForegroundAttention", "None"]
    ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d' for multiclass segmentation
    DEVICE = 'cuda'

    # create segmentation model with pretrained encoder
    model = smp.Unet(
        encoder_name=ENCODER,
        encoder_weights=ENCODER_WEIGHTS,
        classes=len(CLASSES),
        activation=ACTIVATION,

    )

    preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)

    train_dataset = Dataset(
        TRAIN_IMG_DIR,
        TRAIN_MASKS_DIR,
        augmentation=get_training_augmentation(),
        preprocessing=get_preprocessing(preprocessing_fn),
        classes=CLASSES,
    )

    valid_dataset = Dataset(
        VALID_IMG_DIR,
        VALID_MASKS_DIR,
        augmentation=get_validation_augmentation(),
        preprocessing=get_preprocessing(preprocessing_fn),
        classes=CLASSES,
    )

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)
    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)


    # ['Animal', 'NonMaskingBackground', "MaskingBackground", "NonMaskingForegroundAttention", "None"]
    loss = smp.utils.losses.DiceLoss()
    metrics = [
        utils.metrics.IoU(threshold=0.5),
    ]

    optimizer = torch.optim.Adam([
        dict(params=model.parameters(), lr=0.0001),
    ])

    train_epoch = smp.utils.train.TrainEpoch(
        model,
        loss=loss,
        metrics=metrics,
        optimizer=optimizer,
        device=DEVICE,
        verbose=True,
    )

    valid_epoch = smp.utils.train.ValidEpoch(
        model,
        loss=loss,
        metrics=metrics,
        device=DEVICE,
        verbose=True,
    )

    max_score = 0

    for i in range(0, 50):

        print('\nEpoch: {}'.format(i))
        train_logs = train_epoch.run(train_loader)
        valid_logs = valid_epoch.run(valid_loader)

        # do something (save model, change lr, etc.)
        if max_score < valid_logs['iou_score']:
            max_score = valid_logs['iou_score']
            torch.save(model, './best_model.pth')
            print('Model saved!')

        if i == 25:
            optimizer.param_groups[0]['lr'] = 1e-5
            print('Decrease decoder learning rate to 1e-5!')

    best_model = torch.load('./best_model.pth')

    test_dataset = Dataset(
        TEST_IMG_DIR,
        TEST_MASKS_DIR,
        augmentation=get_validation_augmentation(),
        preprocessing=get_preprocessing(preprocessing_fn),
        classes=CLASSES,
    )

    test_dataloader = DataLoader(test_dataset)

    test_epoch = smp.utils.train.ValidEpoch(
        model=best_model,
        loss=loss,
        metrics=metrics,
        device=DEVICE,
    )

    logs = test_epoch.run(test_dataloader)

    test_dataset_vis = Dataset(
        TEST_IMG_DIR, TEST_MASKS_DIR,
        classes=CLASSES,
    )

    save_results(test_dataset_vis, test_dataset, best_model)


if __name__ == '__main__':
    from multiprocessing import freeze_support

    freeze_support()
    main()